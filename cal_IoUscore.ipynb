{"cells":[{"cell_type":"markdown","id":"f1d7b0fa","metadata":{"id":"f1d7b0fa"},"source":["# Import Library"]},{"cell_type":"code","execution_count":null,"id":"faa81b90","metadata":{"id":"faa81b90"},"outputs":[],"source":["from sklearn.model_selection import StratifiedKFold\n","\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import numpy as np\n","import os\n","import glob\n","import cv2\n","import pandas as pd\n","import seaborn as sns"]},{"cell_type":"markdown","id":"c91a8095","metadata":{"id":"c91a8095"},"source":["# Data load"]},{"cell_type":"code","source":["BASE_DIR = os.path.dirname(os.path.abspath(__file__))"],"metadata":{"id":"QeaU22d35xuI"},"id":"QeaU22d35xuI","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"8188228b","metadata":{"id":"8188228b"},"outputs":[],"source":["img_height = 256\n","img_width = 256\n","batch_size = 32\n","\n","# 19 categories\n","label_list = ['skin', 'nose', 'eye_g'\n","              , 'l_eye', 'r_eye', 'l_brow'\n","              , 'r_brow', 'l_ear', 'r_ear'\n","              , 'mouth', 'u_lip', 'l_lip'\n","              , 'hair', 'hat', 'ear_r'\n","              , 'neck_l', 'neck', 'cloth']\n","\n","color_list = [[0, 0, 0]\n","              , [204, 0, 0], [76, 153, 0], [204, 204, 0]\n","              , [51, 51, 255], [204, 0, 204], [0, 255, 255]\n","              , [255, 204, 204], [102, 51, 0], [255, 0, 0]\n","              , [102, 204, 0], [255, 255, 0], [0, 0, 153]\n","              , [0, 0, 204], [255, 51, 153], [0, 204, 204]\n","              , [0, 51, 0], [255, 153, 51], [0, 204, 0]]\n","\n","face_labels = [0,2,4,5,6,7,10,11,12]"]},{"cell_type":"code","execution_count":null,"id":"3ae53ebc","metadata":{"id":"3ae53ebc"},"outputs":[],"source":["def crop_face_region(path, margin=10, target_size=256):\n","    # load the image\n","    img = cv2.imread(path)\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    gray[gray != 0] = 255\n","\n","    # find border pixel\n","    border = np.where(gray == 255)\n","    x_min, x_max = border[1].min(), border[1].max()\n","    y_min, y_max = border[0].min(), border[0].max()\n","\n","    # crop the target region\n","    crop = img[y_min:y_max+1, x_min:x_max+1]\n","    crop_size = max(x_max-x_min, y_max-y_min)\n","\n","    canvas = np.zeros((crop_size+10, crop_size+10)).astype(np.uint8)\n","\n","    center_x, center_y = canvas.shape[0]//2, canvas.shape[0]//2\n","    size_y, size_x = (y_max-y_min)//2, (x_max-x_min)//2\n","    lefttop_point_y, lefttop_point_x = center_y-size_y, center_x-size_x\n","    canvas[lefttop_point_y:lefttop_point_y+(y_max-y_min)+1\n","           , lefttop_point_x:lefttop_point_x+(x_max-x_min)+1] = crop[:,:,0]\n","    canvas = cv2.resize(canvas, (target_size, target_size), interpolation = cv2.INTER_NEAREST)\n","\n","    return canvas\n","\n","def label_changing(anno_mask, mode=\"celeb2class\"):\n","    if mode == \"celeb2class\":\n","        for cls_idx, org_idx in enumerate(face_labels):\n","            anno_mask[anno_mask == org_idx] = cls_idx\n","    elif mode == \"class2celeb\":\n","        for cls_idx in range(8, -1, -1):\n","            org_idx = face_labels[cls_idx]\n","            anno_mask[anno_mask == cls_idx] = org_idx\n","    return anno_mask\n","\n","def mask_coloring(anno_mask, ver='face'):\n","    anno_mask = cv2.cvtColor(anno_mask, cv2.COLOR_GRAY2RGB)\n","    canvas = np.zeros_like(anno_mask)\n","    for i, c in enumerate(color_list):\n","        if ver == 'all' :\n","            canvas[np.where((anno_mask == (i,i,i)).all(axis = 2))] = c\n","        elif ver == 'face' :\n","            if i in face_labels :\n","                canvas[np.where((anno_mask == (i,i,i)).all(axis = 2))] = c\n","    return canvas\n","\n","def map_func(path):\n","    name = path.numpy().decode()\n","    name = name.split('\\\\')[-1]\n","    mask_cropped = crop_face_region(path.numpy().decode())\n","    mask_label = label_changing(mask_cropped, mode= \"celeb2class\")\n","    return name, mask_label"]},{"cell_type":"code","execution_count":null,"id":"1887d11c","metadata":{"id":"1887d11c"},"outputs":[],"source":["s_dataset = os.path.join(BASE_DIR, \"Datasets/GENKI-seg/smile/labels/face_*.png\")\n","ns_dataset = os.path.join(BASE_DIR, \"Datasets/GENKI-seg/non_smile/labels/face_*.png\")\n","\n","s_dataset_path = glob.glob(s_dataset)\n","s_dataset_path.sort(reverse=True)\n","s_dataset_path = tf.data.Dataset.from_tensor_slices(s_dataset_path)\n","\n","ns_dataset_path = glob.glob(ns_dataset)\n","ns_dataset_path.sort(reverse=True)\n","ns_dataset_path = tf.data.Dataset.from_tensor_slices(ns_dataset_path)\n","\n","s_mask_ds = s_dataset_path.map(lambda item: tf.py_function(map_func, [item], [tf.string, tf.int32]))\n","ns_mask_ds = ns_dataset_path.map(lambda item: tf.py_function(map_func, [item], [tf.string, tf.int32]))"]},{"cell_type":"code","execution_count":null,"id":"a2868387","metadata":{"id":"a2868387"},"outputs":[],"source":["smile_mask_names = np.array([name.numpy().decode('utf-8') for name, data in s_mask_ds])\n","smile_mask_ds_arr = np.array([data for name, data in s_mask_ds])\n","smile_target_arr = np.array([1 for _ in range(len(s_mask_ds))])\n","\n","nonsmile_mask_names = np.array([name.numpy().decode('utf-8') for name, data in ns_mask_ds])\n","nonsmile_mask_ds_arr = np.array([data for name, data in ns_mask_ds])\n","nonsmile_target_arr = np.array([0 for _ in range(len(ns_mask_ds))])"]},{"cell_type":"markdown","id":"80796389","metadata":{"id":"80796389"},"source":["* fold 나누기"]},{"cell_type":"code","execution_count":null,"id":"18d478f3","metadata":{"id":"18d478f3"},"outputs":[],"source":["NUM_FOLDS = 5\n","skf = StratifiedKFold(n_splits=NUM_FOLDS)"]},{"cell_type":"code","execution_count":null,"id":"718dbe2c","metadata":{"id":"718dbe2c"},"outputs":[],"source":["target_fold_num = 3\n","\n","smile_mask_folds = skf.split(smile_mask_ds_arr, smile_target_arr)\n","nonsmile_mask_folds = skf.split(nonsmile_mask_ds_arr, nonsmile_target_arr)\n","\n","temp_fold = 1\n","for (s_train_idx, s_val_idx), (ns_train_idx, ns_val_idx) in zip(smile_mask_folds, nonsmile_mask_folds):\n","    if temp_fold == target_fold_num:\n","\n","        #### train data\n","        smile_mask_names = smile_mask_names[s_train_idx]\n","        smile_masks = smile_mask_ds_arr[s_train_idx]\n","        smile_mask_target = smile_target_arr[s_train_idx]\n","\n","        nonsmile_mask_names = nonsmile_mask_names[ns_train_idx]\n","        nonsmile_masks = nonsmile_mask_ds_arr[ns_train_idx]\n","        nonsmile_mask_target = nonsmile_target_arr[ns_train_idx]\n","\n","        smile_masks = tf.data.Dataset.from_tensor_slices(smile_masks)\n","        smile_masks = smile_masks.batch(batch_size)\n","\n","        nonsmile_masks = tf.data.Dataset.from_tensor_slices(nonsmile_masks)\n","        nonsmile_masks = nonsmile_masks.batch(batch_size)\n","\n","        #### validataion data\n","        smile_mask_names_val = smile_mask_names[s_val_idx]\n","        smile_masks_val = smile_mask_ds_arr[s_val_idx]\n","        smile_mask_target_val = smile_target_arr[s_val_idx]\n","\n","        nonsmile_mask_names_val = nonsmile_mask_names[ns_val_idx]\n","        nonsmile_masks_val = nonsmile_mask_ds_arr[ns_val_idx]\n","        nonsmile_mask_target_val = nonsmile_target_arr[ns_val_idx]\n","\n","        smile_masks_val = tf.data.Dataset.from_tensor_slices(smile_masks_val)\n","        smile_masks_val = smile_masks_val.batch(batch_size)\n","\n","        nonsmile_masks_val = tf.data.Dataset.from_tensor_slices(nonsmile_masks_val)\n","        nonsmile_masks_val = nonsmile_masks_val.batch(batch_size)\n","        break\n","\n","    temp_fold += 1"]},{"cell_type":"markdown","id":"a01a1b89","metadata":{"id":"a01a1b89"},"source":["# Set DL Model"]},{"cell_type":"code","execution_count":null,"id":"69f07afa","metadata":{"id":"69f07afa"},"outputs":[],"source":["OUTPUT_CHANNEL = 9\n","SAVE_DIR = os.path.join(BASE_DIR, \"models/cVAE\")\n","\n","class CVAE(tf.keras.Model):\n","    def __init__(self\n","                 , latent_dim\n","                 , fold_num\n","                 , optimizer=None):\n","\n","        super(CVAE, self).__init__()\n","        self.output_dir = SAVE_DIR\n","        self.latent_dim = latent_dim\n","        self.fold_num = fold_num\n","        self.optimizer = optimizer\n","\n","        self.encoder = tf.keras.Sequential(\n","            [\n","                tf.keras.layers.InputLayer(input_shape=(256, 256, 1)),\n","                tf.keras.layers.Conv2D(\n","                    filters=8, kernel_size=3, strides=(2, 2), padding='same', activation='relu'),\n","                tf.keras.layers.Conv2D(\n","                    filters=16, kernel_size=3, strides=(2, 2), padding='same', activation='relu'),\n","                tf.keras.layers.Conv2D(\n","                    filters=32, kernel_size=3, strides=(2, 2), padding='same', activation='relu'),\n","                tf.keras.layers.Conv2D(\n","                    filters=64, kernel_size=3, strides=(2, 2), padding='same', activation='relu'),\n","                tf.keras.layers.Conv2D(\n","                    filters=128, kernel_size=3, strides=(2, 2), padding='same', activation='relu'),\n","                tf.keras.layers.Flatten(),\n","                # No activation\n","                tf.keras.layers.Dense(latent_dim + latent_dim),\n","            ]\n","        )\n","\n","        self.decoder = tf.keras.Sequential(\n","            [\n","                tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n","                tf.keras.layers.Dense(units=8*8*128, activation=tf.nn.relu),\n","                tf.keras.layers.Reshape(target_shape=(8, 8, 128)),\n","                tf.keras.layers.Conv2DTranspose(\n","                    filters=128, kernel_size=3, strides=2, padding='same',\n","                    activation='relu'),\n","                tf.keras.layers.Conv2DTranspose(\n","                    filters=64, kernel_size=3, strides=2, padding='same',\n","                    activation='relu'),\n","                tf.keras.layers.Conv2DTranspose(\n","                    filters=32, kernel_size=3, strides=2, padding='same',\n","                    activation='relu'),\n","                tf.keras.layers.Conv2DTranspose(\n","                    filters=16, kernel_size=3, strides=2, padding='same',\n","                    activation='relu'),\n","                tf.keras.layers.Conv2DTranspose(\n","                    filters=8, kernel_size=3, strides=2, padding='same',\n","                    activation='relu'),\n","                # No activation\n","                tf.keras.layers.Conv2D(OUTPUT_CHANNEL, (1, 1)),\n","            ]\n","        )\n","\n","        self.checkpoints = tf.train.Checkpoint(encoder=self.encoder,\n","                                               decoder=self.decoder,\n","                                               optimizer = self.optimizer)\n","\n","    @tf.function\n","    def sample(self, eps=None):\n","        if eps is None:\n","            eps = tf.random.normal(shape=(100, self.latent_dim))\n","\n","        return self.decode(eps, apply_softmax=True)\n","\n","    def encode(self, x):\n","        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n","        return mean, logvar\n","\n","    def reparameterize(self, mean, logvar, mode='train'):\n","        if mode == 'train':\n","            eps = tf.random.normal(shape=mean.shape)\n","        elif mode == \"inference\":\n","            eps = tf.random.normal(shape=mean.shape, seed=1)\n","        return eps * tf.exp(logvar * .5) + mean\n","\n","    def decode(self, z, apply_softmax=False):\n","        logits = self.decoder(z)\n","        if apply_softmax:\n","            probs = tf.nn.softmax(logits)\n","            return probs\n","        return logits\n","\n","    def save_models(self):\n","        self.checkpoints.save(file_prefix = '{}/fold_{}/checkpoints/ckpt'.format(self.output_dir\n","                                                                                , self.fold_num))\n","\n","    def load_models(self, idx=None):\n","        if idx==None:\n","            ckpt_prefix = '{}/checkpoints/fold_{}'.format(self.output_dir, self.fold_num)\n","            self.checkpoints.restore(tf.train.latest_checkpoint(ckpt_prefix))\n","        else:\n","            self.checkpoints.restore(file_prefix = '{}/fold_{}/checkpoints/ckpt-{}'.format(self.output_dir\n","                                                                                           , self.fold_num\n","                                                                                           , idx))\n","        self.encoder = self.checkpoints.encoder\n","        self.decoder = self.checkpoints.decoder\n","        self.optimizer = self.checkpoints.optimizer"]},{"cell_type":"code","execution_count":null,"id":"52c5f2f9","metadata":{"id":"52c5f2f9","outputId":"624336a1-87c2-4f44-908f-171e0587cd6f"},"outputs":[{"data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x23c6fa6bdc0>"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["vae_model = CVAE(latent_dim=10\n","            , fold_num = target_fold_num\n","            , optimizer = tf.keras.optimizers.Adam(1e-4))\n","\n","# vae_model.load_models()\n","vae_model.load_weights('{}/checkpoints/fold_{}/ckpt-10'.format(SAVE_DIR, target_fold_num))"]},{"cell_type":"markdown","id":"dc7cca21","metadata":{"id":"dc7cca21"},"source":["# Calculate IoU score"]},{"cell_type":"code","execution_count":null,"id":"be04e330","metadata":{"id":"be04e330"},"outputs":[],"source":["class CalAnomalyScore:\n","    def __init__(self, model):\n","        self.model = model\n","\n","    def iou_calculation(self, org_cmps, seg_cmps):\n","        iou = []\n","        # Calculate IoU score per facial component\n","        for org_c, seg_c in zip(org_cmps, seg_cmps):\n","            if np.all(org_c == 0) and np.all(seg_c == 0):\n","                iou.append(0)\n","            else:\n","                intersection = cv2.countNonZero(np.bitwise_and(org_c, seg_c))\n","                union = cv2.countNonZero(np.bitwise_or(org_c, seg_c))\n","                iou.append(intersection/union)\n","\n","        return iou\n","\n","    def select_and_score(self, dataset, target_labels=[6], mode='iou'):\n","        scores = []\n","        for label in dataset:\n","            mean, logvar = self.model.encode(tf.expand_dims(label, axis=-1))\n","            z = self.model.reparameterize(mean, logvar)\n","            predictions = self.model.sample(z)\n","            predictions = tf.argmax(predictions, axis=-1)\n","\n","            for org, seg in zip(label, predictions):\n","                org_cmps, seg_cmps = [], []\n","                for idx in target_labels:\n","                    o_base, s_base = np.zeros_like(org), np.zeros_like(seg)\n","                    o_base[org == idx] = 1\n","                    s_base[seg == idx] = 1\n","                    org_cmps.append(o_base)\n","                    seg_cmps.append(s_base)\n","\n","                # Calculate IoU score per facial component\n","                if mode == 'iou':\n","                    scores.append(self.iou_calculation(org_cmps, seg_cmps))\n","                elif mode == 'spsim':\n","                    scores.append(self.shape_similarity(org_cmps, seg_cmps))\n","\n","        return scores\n","\n","    def plot_anomaly_score(self, data1, data2, metric_name, fig_title):\n","        sns.set(rc = {'figure.figsize':(20,5)})\n","        sns.set(font_scale = 2)\n","        sns.set_style(\"whitegrid\")\n","        sns.histplot(data=data1[0], kde=True, label=data1[1], color=\"green\", element='step')\n","        sns.histplot(data=data2[0], kde=True, label=data2[1], color=\"purple\", element='step')\n","        plt.xlabel('{} Score'.format(metric_name))\n","        plt.legend()\n","        plt.title(fig_title)\n","        plt.axvline(x=np.mean(data1[0]), color='green', linestyle='--')\n","        plt.axvline(x=np.mean(data2[0]), color=\"purple\", linestyle='--')\n","\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"id":"be5bf6db","metadata":{"id":"be5bf6db"},"outputs":[],"source":["cas = CalAnomalyScore(model = vae_model)"]},{"cell_type":"code","execution_count":null,"id":"78cf447a","metadata":{"id":"78cf447a","outputId":"31518796-22f0-42a4-894d-33b7276df28d"},"outputs":[{"name":"stdout","output_type":"stream","text":["               0         0         1         2         3         4         5  \\\n","0  face_1729.png  0.895676  0.560976  0.652893  0.718363  0.845039  0.615848   \n","1  face_1728.png  0.895823  0.492386  0.486967  0.637989  0.787647  0.753160   \n","2  face_1727.png  0.894855  0.622103  0.600939  0.592186  0.731492  0.860233   \n","3  face_1726.png  0.909529  0.629902  0.588491  0.666316  0.706568  0.333333   \n","4  face_1725.png  0.868358  0.625800  0.736620  0.578577  0.590186  0.686787   \n","\n","          6         7  \n","0  0.664569  0.868949  \n","1  0.656355  0.790425  \n","2  0.831717  0.805712  \n","3  0.528796  0.722964  \n","4  0.473467  0.819813  \n","               0         0         1         2         3         4         5  \\\n","0  face_2162.png  0.944961  0.627551  0.398977  0.673645  0.728736  0.889650   \n","1  face_2161.png  0.924706  0.644970  0.504298  0.704861  0.785248  0.819780   \n","2  face_2160.png  0.882685  0.816358  0.748188  0.797289  0.692573  0.875870   \n","3  face_2159.png  0.933083  0.776699  0.876374  0.625098  0.562044  0.908292   \n","4  face_2158.png  0.927257  0.671569  0.731211  0.797353  0.762865  0.000000   \n","\n","          6         7  \n","0  0.837825  0.912049  \n","1  0.625736  0.798848  \n","2  0.729730  0.851882  \n","3  0.839057  0.881537  \n","4  0.788148  0.826546  \n","               0         0         1         2         3         4         5  \\\n","0  face_2162.png  0.944301  0.623410  0.397959  0.671798  0.722477  0.893226   \n","1  face_2161.png  0.925589  0.649606  0.502857  0.708014  0.780679  0.817062   \n","2  face_2160.png  0.883419  0.816358  0.736086  0.798189  0.691097  0.875726   \n","3  face_2159.png  0.932498  0.777060  0.881868  0.625589  0.545058  0.908941   \n","4  face_2158.png  0.929339  0.664706  0.729443  0.799834  0.744651  0.000000   \n","\n","          6         7  \n","0  0.840000  0.912366  \n","1  0.634907  0.793665  \n","2  0.732046  0.851761  \n","3  0.841727  0.881503  \n","4  0.796283  0.824385  \n","               0         0         1         2         3         4         5  \\\n","0  face_2162.png  0.943677  0.631043  0.401535  0.672010  0.728111  0.891313   \n","1  face_2161.png  0.924416  0.652778  0.514620  0.706416  0.780026  0.817143   \n","2  face_2160.png  0.883115  0.815789  0.740541  0.797027  0.690453  0.877254   \n","3  face_2159.png  0.932498  0.775444  0.884615  0.627451  0.562044  0.907443   \n","4  face_2158.png  0.927158  0.670588  0.730327  0.803824  0.760612  0.000000   \n","\n","          6         7  \n","0  0.836136  0.911828  \n","1  0.624895  0.797351  \n","2  0.728958  0.851473  \n","3  0.837340  0.880925  \n","4  0.786667  0.827136  \n","               0         0         1         2         3         4         5  \\\n","0  face_2162.png  0.942521  0.630102  0.397959  0.672020  0.722477  0.891479   \n","1  face_2161.png  0.925868  0.646943  0.505747  0.707810  0.777415  0.815303   \n","2  face_2160.png  0.882558  0.813170  0.748188  0.799096  0.693556  0.875362   \n","3  face_2159.png  0.932007  0.778317  0.887052  0.630878  0.552975  0.908941   \n","4  face_2158.png  0.929149  0.667973  0.734099  0.804817  0.751729  0.000000   \n","\n","          6         7  \n","0  0.840636  0.911069  \n","1  0.620370  0.796378  \n","2  0.726641  0.850143  \n","3  0.844810  0.880670  \n","4  0.795252  0.826875  \n","WARNING:tensorflow:Unresolved object in checkpoint: (root).save_counter\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n","WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"]}],"source":["smile_mask_folds = skf.split(smile_mask_ds_arr, smile_target_arr)\n","nonsmile_mask_folds = skf.split(nonsmile_mask_ds_arr, nonsmile_target_arr)\n","\n","fold_no = 1\n","for (s_train_idx, s_val_idx), (ns_train_idx, ns_val_idx) in zip(smile_mask_folds, nonsmile_mask_folds):\n","\n","    #### train data\n","    smile_names = smile_mask_names[s_train_idx]\n","    smile_masks = smile_mask_ds_arr[s_train_idx]\n","    smile_mask_target = smile_target_arr[s_train_idx]\n","\n","    nonsmile_names = nonsmile_mask_names[ns_train_idx]\n","    nonsmile_masks = nonsmile_mask_ds_arr[ns_train_idx]\n","    nonsmile_mask_target = nonsmile_target_arr[ns_train_idx]\n","\n","    smile_masks = tf.data.Dataset.from_tensor_slices(smile_masks)\n","    smile_masks = smile_masks.batch(batch_size)\n","\n","    nonsmile_masks = tf.data.Dataset.from_tensor_slices(nonsmile_masks)\n","    nonsmile_masks = nonsmile_masks.batch(batch_size)\n","\n","    #### validataion data\n","    smile_names_val = smile_mask_names[s_val_idx]\n","    smile_masks_val = smile_mask_ds_arr[s_val_idx]\n","    smile_mask_target_val = smile_target_arr[s_val_idx]\n","\n","    nonsmile_names_val = nonsmile_mask_names[ns_val_idx]\n","    nonsmile_masks_val = nonsmile_mask_ds_arr[ns_val_idx]\n","    nonsmile_mask_target_val = nonsmile_target_arr[ns_val_idx]\n","\n","    smile_masks_val = tf.data.Dataset.from_tensor_slices(smile_masks_val)\n","    smile_masks_val = smile_masks_val.batch(batch_size)\n","\n","    nonsmile_masks_val = tf.data.Dataset.from_tensor_slices(nonsmile_masks_val)\n","    nonsmile_masks_val = nonsmile_masks_val.batch(batch_size)\n","\n","    sw_iou_train = cas.select_and_score(smile_masks, target_labels=range(1,9))\n","    nsw_iou_train = cas.select_and_score(nonsmile_masks, target_labels=range(1,9))\n","\n","    sw_iou_val = cas.select_and_score(smile_masks_val, target_labels=range(1,9))\n","    nsw_iou_val = cas.select_and_score(nonsmile_masks_val, target_labels=range(1,9))\n","\n","    train_s_name = pd.DataFrame(smile_names)\n","    train_ns_name = pd.DataFrame(nonsmile_names)\n","\n","    train_sw_iou = pd.DataFrame(sw_iou_train)\n","    train_nsw_iou = pd.DataFrame(nsw_iou_train)\n","\n","    val_s_name = pd.DataFrame(smile_names_val)\n","    val_ns_name = pd.DataFrame(nonsmile_names_val)\n","\n","    val_sw_iou = pd.DataFrame(sw_iou_val)\n","    val_nsw_iou = pd.DataFrame(nsw_iou_val)\n","\n","\n","    train_sw = pd.concat([train_s_name, train_sw_iou], axis=1)\n","    train_nsw = pd.concat([train_ns_name, train_nsw_iou], axis=1)\n","\n","    val_sw = pd.concat([val_s_name, val_sw_iou], axis=1)\n","    val_nsw = pd.concat([val_ns_name, val_nsw_iou], axis=1)\n","\n","    train_sw.to_csv('./iou/train_sw_iou(fold{})_withName.csv'.format(fold_no), header=False, index=False)\n","    train_nsw.to_csv('./iou/train_nsw_iou(fold{})_withName.csv'.format(fold_no), header=False, index=False)\n","\n","    val_sw.to_csv('./iou/test_sw_iou(fold{})_withName.csv'.format(fold_no), header=False, index=False)\n","    val_nsw.to_csv('./iou/test_nsw_iou(fold{})_withName.csv'.format(fold_no), header=False, index=False)\n","\n","    print(train_sw.head())\n","    fold_no += 1"]},{"cell_type":"code","execution_count":null,"id":"7c3c5870","metadata":{"id":"7c3c5870"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"tf26_py38","language":"python","name":"tf26_py38"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}